<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://syeda5688.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://syeda5688.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-11T18:50:00+00:00</updated><id>https://syeda5688.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Syed Rizvi. </subtitle><entry><title type="html">Learnings From Undergraduate Research</title><link href="https://syeda5688.github.io/blog/2025/undergraduate-research-learnings/" rel="alternate" type="text/html" title="Learnings From Undergraduate Research"/><published>2025-01-11T02:00:00+00:00</published><updated>2025-01-11T02:00:00+00:00</updated><id>https://syeda5688.github.io/blog/2025/undergraduate-research-learnings</id><content type="html" xml:base="https://syeda5688.github.io/blog/2025/undergraduate-research-learnings/"><![CDATA[<p>This blog post is a collection of thoughts and learnings I gathered from my experience as an undergraduate research student in Computer Science, mainly focused around what worked well for me and helped me get into graduate school. I sometimes have friends or younger undergraduate students asking me for advice about PhD applications and doing research in general as an undergraduate student. This post contains my thoughts about a few different topics which I hope will be helpful for other students considering graduate school: (i) Whether or not to pursue a PhD, (ii) what are different ways to reach out to professors as an undergraduate, and (iii) how to build up a strong research profile for graduate applications.</p> <p>There are many great tutorials and blog posts that give more comprehensive steps or plans for reaching graduate school, which I highly recommend looking into. This post will contain additional points which I found helpful when told to me directly as an undergraduate research student. As always, feedback is welcome!</p> <h1 id="phase-1-deciding-on-graduate-school">Phase 1: Deciding on Graduate School</h1> <p>The first step along the journey to graduate school is deciding whether or not you want to pursue higher education beyond a bachelor’s or master’s degree. There is an amazing preliminary discussion about this in Andrej Karpathy’s <a href="https://karpathy.github.io/2016/09/07/phd/">“Survival Guide to a PhD”</a>, which I highly recommend checking out. To reiterate a few important points: A PhD requires lots of mental stamina and curiosity about your research subject - it is very much a marathon, not a race. You will receive a lot of critical feedback on your research work, writing, and presentational skills, but you also get the chance to work alongside many brilliant students and professors, and learn things from them. If educational achievement, curiosity in your subject, or the desire to build something new drives you, it could be signs that pursuing a PhD would be worthwhile.</p> <p>One thing which I will add is that if you plan to go into industry and salary is a major financial consideration for you doing a PhD, then it may be worth looking more into the salary range of people with and without PhD degrees in your field first. Depending on the subject and department, a PhD can take around 6 years to complete, which is a significant period of time in your life. Someone who completes undergrad in the same year as you, enters the workforce, and receives several promotions by the time you enter the workforce with a PhD in hand may make a higher salary than you, which was not immediately obvious to me as an undergraduate. This may or may not be a deciding factor for you; for some positions (e.g. research scientist positions in AI), a PhD is commonly required or expected as qualification for the job anyways, which might make the decision for you if you want a similar position.</p> <p>For my own part, a desire to pursue higher education and a genuine curiosity about Computer Science (CS) and Artificial Intelligence (AI) was already set in my head by the time I was in my second year of undergrad. Both of my parents had earned master’s degrees after immigrating into the U.S., and I saw PhD programs as a convenient way to pursue higher education which would be funded if I did research, and which would get me closer to my goal of becoming an AI research scientist (which often required a PhD anyways).</p> <h1 id="phase-2-how-do-i-get-started">Phase 2: How Do I Get Started?</h1> <p>If you have decided to give research a try, there comes a difficult phase where you do not have experience in your field or subject, yet need to convince others to give you an opportunity so that you can begin learning and receiving research mentorship. This phase can be a bit daunting, but the good news is that there are many things you can do on your own to learn and develop your profile before you approach research professors and labs for positions!</p> <h3 id="things-to-invest-time-in-at-the-beginning">Things to invest time in at the beginning</h3> <ol> <li>University courses in your subject <ul> <li>The most straightforward course of action is to take courses at your university or online as early as possible in your subject of interest. Be active in your course; the course material and assignments will introduce you to important concepts in your subject, however there will also be opportunities to do projects which can develop into major profile-building projects depending on how much effort you invest in them. The professor teaching the course may also be involved in their own research, or may know of other professors in the same department who works in the general subject, which can be a great way for you to find initial professors to work with. Making a good impression and attending lectures/office hours is a good way to get exposure to professors in person.</li> <li>The goal of taking courses is to get basic understanding of major concepts needed to work in your field, and to add course projects or other activities associated with the course to your CV.</li> </ul> </li> <li>Online courses and certifications (Coursera, Udemy) <ul> <li>In addition to university courses (or possibly as a supplement), you can also do online courses and certifications to build more skills and credentials to advertise on your profile. These courses can either be free or cost some money - I highly recommend Coursera for Computer Science courses. There is some debate over the value of these online courses, however what is not debatable is that going out of your way to do extra certifications and improve your skills demonstrates some measure of self-motivation, which itself is valuable if you can speak in interviews about how you are trying to improve your skills. These online certifications also often have you do additional projects, which gives you additional opportunities to add projects to your profile.</li> <li>Looking back at my own career development, one hidden benefit of doing online courses early on was that it allowed me to take courses and learn Machine Learning concepts years before I would normally take them in my degree program. By doing Machine Learning and Data Science certifications in my freshman year of undergrad, I was two years ahead of taking the same courses at my university (based on course progressions in my degree plan), which taught me just enough to begin working in research labs earlier on in undergrad. In general, exposure to your subject of interest early on and as often as possible can be a big accelerator, since you need time to develop your research profile during undergrad.</li> </ul> </li> <li>Personal projects <ul> <li>Personal projects can be extensions of projects which you worked on in courses, or can be completely separate projects which you pursue in your spare time. Personal projects demonstrate your motivation to get hands-on with building something in your research subject - for Computer Science, this usually involves software applications, and for AI this usually involves training Machine Learning (ML) models on datasets which you are interested in. In general, if you are interested in a particular dataset, domain, model architecture, or concept, consider doing a project using it to put on your CV. This can be especially beneficial if you reach out in the future to research labs which use similar models or techniques.</li> <li>Coming up with ideas for projects can be difficult - in general, I would recommend picking something you are genuinely interested in and would be willing to work on for potentially several months. That will increase your chances that you will finish and deploy the project somewhere that is publicly visible, which is what is needed to build up your CV with hands-on experience in the beginning. For my first two projects as a beginner CS student, I created a hurricane tracking mobile application and a calorie/nutrient tracking app for my phone, both of which had avenues where I could integrate AI through chat assistants or image recognition.</li> <li>Personal projects is another phase where you will realize whether or not you like working and getting hands-on with your research subject, which can inform you on whether you have the stamina to work and research in some topic deeply for many years without getting burned out. You should aim to create open-source code or presentations which you can share with others who are interested in seeing what you did with your project.</li> </ul> </li> <li>Hackathons or competitions <ul> <li>Aside from being fun competitions, hackathons can be great launching points for projects and career development opportunities for Computer Science undergraduates (and other majors!). They offer challenges for programming teams to address through software and/or hardware projects, which adds projects to your CV and exposes you to a large variety of programming languages, software, topics, and people. Recruiters from companies and research labs also attend hackathons, which can in some cases lead directly to internships and work experience.</li> <li>Hackathons also bring a great attitude of self-motivated project building and competitive programming, which is a great mentality to carry and looks good on resumes.</li> <li><code class="language-plaintext highlighter-rouge">As a fun personal anecdote: I first got into Machine Learning through an online hackathon in Summer of 2020. A friend of mines convinced me to signup for a Machine Learning hackathon since it was online (during the COVID-19 pandemic) and had a large prize pool sponsored by AWS &amp; NVIDIA. The hackathon lasted 3 weeks long was focused on anomaly detection in environmental sensor data gathered from greenhouses in Seattle. My friend ended up not participating in the hackathon because of other events she wanted to attend, but I decided to give it a go anyways and went in alone. I spent the first week studying libraries like Numpy and Pandas (I was quite new to Python at the time), and spent the second week reviewing basic neural networks and ML libraries like Pytorch and Keras. Finally, in the third week, I tinkered around with the autoencoder tutorials which the hackathon organizers from AWS had posted as a starting point for projects. After some hyperparameter tuning, I was able to get decent reconstruction of the sensor data on the validation set, and I then focused on trying to use autoencoder reconstructions to flag anomalous sensor readings. University courses were starting soon in August, so I quickly wrote an evaluation function to detect anomalies based the duration and magnitude of autoencoder reconstruction error, with the idea that the autoencoder will have a difficult time reconstructing anomalous data points that do not occur in clean training data. I recorded a presentation and submitted it at the deadline, and went off for another semester of CS courses. To my surprise, an email came back in a week saying that I had placed third in the competition! The look on my father’s face when he learned that I had earned some prize money by sitting at home for three weeks alone made the whole experience worth it for me, and I soon after joined a Computer Vision research lab at my university as an undergraduate research assistant.</code></li> </ul> </li> <li>Student organizations <ul> <li>Student organizations can be a great way to get around other students with the same mindset as you, and bring you in contact with more opportunities than you will be able to find alone. Depending on the subject and field, these organizations can potentially connect you with career development opportunities (career fairs, mixers, alumni chats) as well as tutoring sessions and events to attend in your subject.</li> </ul> </li> </ol> <p>Investing time in 3-4 of these as an undergraduate demonstrates a lot of self-motivation, and it strengthens your resume before you have to step in front of recruiters or research labs. In the end, you are always trying to demonstrate that you have the skills and consistent work ethic to stay in a research lab and make progress on research projects and questions which they are interested in. Having multiple of these experiences on your CV demonstrates that you have put in a lot of effort on your own already, and will likely put in a lot of effort if they take a chance and recruit you as an undergraduate research student. It also serves multiple other purposes: (i) you will be exposed to tools, programming languages, software, theory, or other concepts from your field early on, and (ii) if it turns out that you are not as interested in the field as you originally thought, you will likely find that earlier on while trying to motivate yourself to do these things, and you can re-focus on other subjects or career paths.</p> <div style="border: 1px solid #cccccc; padding: 15px; background-color: lightblue; border-radius: 5px;"> One heuristic which worked well for me as an undergraduate was to find people 3-4 years ahead of me in positions which I wanted to reach, and either emulate what they did to reach that point or surround myself with those people as much as I possibly could. In my first and second year of undergrad I knew that I wanted to enter PhD programs in Computer Science and AI research, so my plan looked something like: Find profiles for competitive CS/AI PhD students on LinkedIn at major universities, and emulate what they did as undergraduate students. Surround myself with AI researchers and research students as much as possible Before joining research labs as an undergraduate research student, I did this by attending hackathons with Machine Learning challenges as well as student events related to data science and Machine Learning. After joining my first research lab, this happened by default. Working in-person in a graduate research lab alongside real PhD students directly surrounds you with the people whom you want to emulate, and those students can directly advise you on what worked and didn’t work for them to reach their position. </div> <p>As a final thought on getting started in research, self-motivation and determination can go quite a long way in this phase. Many people I have met in research are extremely self-motivated, and are willing to invest many hours on their own studying and improving themselves in different ways to learn more about their subject of interest. Many people working in AI research actually do not have a Computer Science background, but instead come from Physics, Statistics, Engineering, Biology, and other fields, which goes to show that time investment and demonstrated skillset can have just as much impact as the subject which your degree is titled with.</p> <h3 id="how-to-reach-out-to-research-labs">How to Reach Out to Research Labs</h3> <p>Now that you have some experience and background knowledge about your subject of interest, you come to a point where you need to reach out to professors to try and secure a position as an undergraduate researcher so that you can officially do research with professors and graduate students. This is the most important process to get started as early on as possible as an undergraduate. Outside of professors who you have contact with through courses and events, reaching out will likely involve cold emailing professors asking about positions in their lab. I have seen various forms of cold emailing professors, which frequently goes something like:</p> <div style="border: 1px solid #cccccc; padding: 15px; background-color: #f9f9f9; border-radius: 5px;"> Dear Professor X, <br/> <br/> My name is Y, I am a student currently majoring in Z at the University of AA. My background is in B, and I am currently working on … <br/> <br/> In the past I have worked on C, and I am familiar with D and E as well. I am potentially interested in your work on F, and would love to potentially work on that as a part of your lab. I {do/do not} require any funding, and will be available to work N hours per week. <br/> <br/> I am interested in research positions for Summer/Fall this year, and wanted to inquire about potential research positions within your lab. Please let me know if you are available for a Zoom chat, looking forward to your response! <br/> <br/> Sincerely, <br/> Y </div> <p>While this email is good and can work sometimes with professors, it has a major problem:</p> <ul> <li>In order to give your email an answer, the professor reading this email has to judge based on your online profile alone whether or not to incorporate you as an undergraduate research student in their lab. Professors get numerous emails like these, and unless you are a standout candidate, the chances of the professor feeling strongly enough to respond to your email and offer you a position/interview are slim. This is because it is a large commitment for professors and graduate students to add someone to their lab - new people require training and time from other lab members to get up to speed, and the professor will have to dedicate some of their time to mentor and meet with you consistently.</li> </ul> <p>Furthermore, there are a lot of soft skills which do not show up through an email and an online portfolio - your personality, spoken communication skills, and a lot of other things which you do not find out until you work with someone (understanding research project goals, making powerpoints, scheduling meetings, coding and managing experiments, organizational skills, communicating project updates and issues encountered, writing skills, etc). You want the professor to get to see your skills in these areas as well before making a decision yes/no to pick you up, and you ideally also want a graduate student in their lab to be rooting for you to join the lab (since the graduate student is who you will spend the majority of your time with, and the graduate student will much more quickly convince the professor to add you to their lab if they see your potential beforehand).</p> <p>So how do you get to the point where a professor knows more about you, and a graduate student in their lab is rooting for you to join? Wording your initial email a bit differently can potentially help:</p> <div style="border: 1px solid #cccccc; padding: 15px; background-color: #f9f9f9; border-radius: 5px;"> Dear Professor X, <br/> <br/> My name is Y, I am a student currently majoring in Z at the University of AA. My background is in B, and I am currently working on … <br/> <br/> I came across your lab’s research work on C, and find it quite interesting. I have worked on D and E in the past, and am interested in doing similar research in this area in order to gain more experience. <strong>If possible, I would be highly interested in joining any lab meetings or project meetings in your group related to this research area, to potentially contribute and become involved in the project with one of your graduate students as an assistant.</strong> If the project match works well, I would be available to work N hours per week on the project in collaboration with other students in your lab.<br/> <br/> Please let me know if this is possible, and let me know if you would like more details on my background or would like to schedule an interview to discuss more. Looking forward to your response!<br/> <br/> Best,<br/> Y </div> <p>This email contains much of the same information and intent as the previous email, however the major difference is in the question which the professor has to answer in order to respond to your email:</p> <ul> <li>In this version of the email, all the professor has to agree to do is let you join as 1 additional face in their online or in-person lab meeting. This is a much more easier commitment for the professor to make, and importantly, it delays the point at which the professor has to say an official yes/no until after the professor and several graduate students have had a chance to interact with you in several research meetings. In these few meetings, it will become quickly apparent whether or not you match the research focus of the lab and have the background coding or basic skills to work with them. Advanced skills are not necessary to join a lab, but some indication of commitment and reliability as well as basic skills to conduct research is required. Joining a few lab meetings gives the professor and students a chance to see and evaluate you before having to commit, which can make their decision easier.</li> <li>Doing research into the projects ongoing in research labs helps a lot here - the more you have looked at the labs projects and can mention the ones which suit your background and which you could contribute to, the more the professor will believe that you are genuinely interested in their research and will stick around their lab as a member. <ul> <li>Finding professors by looking on departmental websites and research lab websites can be difficult, especially since research lab websites are rarely updated. If you are finding it difficult to locate labs working on interesting projects, try going in the reverse direction - gather as many interesting papers, projects, and blog posts as you can find which you would be interested to work on, and then track back to what people/research labs released that research.</li> </ul> </li> <li>The email does not need to follow this phrasing specifically, but the general idea is to make the professor and graduate students commit less through email and delay the decision point until after they have interacted with you more in meetings and have (i) seen your potential which they initially notice from your online profile, and (ii) seen some indications of your soft skills, like how you speak and ask questions about research topics during meetings.</li> </ul> <p>In my experience, professors are usually more willing to let potential students join a few meetings to evaluate them. In the first few meetings with new groups, I try to actively do a few things:</p> <ol> <li>Listen carefully about what research projects are being discussed, and how people in the lab communicate research updates and progress. If you cannot understand most of what they are saying, it might be an indication that their research projects are farther from your background experience than you originally thought, or there is a lot of domain knowledge which you will need to pick up to work effectively on their projects.</li> <li>Pay attention to which graduate students are leading meetings. Usually there will be 1 or 2 graduate students actively leading a research project, and potentially more students supporting them depending on how the lab chooses to structure research projects. Over time, you want to get close and become an assistant to one of the lead graduate students - this is the person who can teach you the most about that particular research topic and delegate tasks to you which will automatically get you involved in their research work. <ul> <li>In each lab I worked in as an undergraduate, I worked closely with 1 PhD student who became my mentor in the lab, and who taught me a lot as a prepared myself for graduate school. In exchange, I would contribute however I can to help that PhD student make progress on the project - coding, running experiments, reading and summarizing papers, writing manuscript drafts, or any other work they would be willing to delegate to me.</li> <li>These relationships with mentoring graduate students can be the most beneficial - you will learn a lot directly and indirectly through osmosis from the graduate students in your lab. In addition, the PhD student will most likely be better at managing and finishing projects than you will be as an undergraduate, which increases your chances of being an author on published research papers compared to if you only did solo research projects during undergrad.</li> </ul> </li> </ol> <h1 id="building-up-a-strong-research-profile">Building Up a Strong Research Profile</h1> <p>It is important to get research experience with labs as an undergraduate as early and as much as possible before you enter graduate school. It can be easy to settle into familiar surroundings and continue working in the same lab with the same people, but diversity in experience as an undergraduate can be very beneficial - remember, once you commit to a graduate school and a research lab for your PhD, you have much less mobility than when you are an undergraduate.</p> <p>Here are a few benefits of working in multiple labs as an undergraduate:</p> <ol> <li>You become more experienced as an undergraduate researcher <ul> <li>Working with multiple labs and different people as an undergraduate student exposes you to multiple different working environments and advising styles of different professors and labs. In some labs, you will meet the professor multiple times per week, and work closely with them on ideas (and maybe even implementations), while in other labs you will have a check-in with the professor once a month, and will instead receive more direct feedback from postdocs, other PhD students, and more senior members of the lab. Experiencing multiple advising styles as an undergraduate can help you diagnose what advising style you respond well to, which will inform you when you are selecting a lab to join for your PhD.</li> <li>In addition, by working with multiple research labs, you are increasing your chances of publishing multiple meaningful papers as a first or second author. Not every research project will work out, so increasing your chances by working on multiple things as an undergraduate mitigates your risk. Your goal should be to have a preprint paper, active paper submission, or if you start early enough, accepted paper, with each lab which you worked with by the time you submit PhD applications to universities. If most of your works are still preprints when you apply to graduate school, professors are usually understanding of that, especially if you are the first author of those preprint papers.</li> </ul> </li> <li>You grow your connections <ul> <li>Working in multiple research labs means that you will have worked with multiple professors as an undergraduate student, ideally at several different institutions. This will mean that there are multiple professors at graduate institutions who can write recommendation letters on your behalf and discuss about your qualifications for going to graduate school. It looks strong on an application to have multiple professors rooting for you and your potential as a PhD student.</li> <li>In addition, further into the future, these professors also stay as a part of your network. Relationships in research last very long, and by working with many different people over your career, you greatly increase your chances of being connected to academic/industry positions and other exciting opportunities by your network connections. <ul> <li>A fun way to think about this is to think about the two or three-hop connections in your research network. All the professors and graduate students you directly work with are 1-hop connections. All of the people they have worked with and connections they have made in their own career are your two-hop connections, which very quickly balloons to an enormous number. Links between institutes (e.g. if your PhD advisor is friends with research scientists at a certain company) also form strong connections to companies and institutes in your research network.</li> </ul> </li> </ul> </li> </ol> <div style="border: 1px solid #cccccc; padding: 15px; background-color: lightblue; border-radius: 5px;"> As a fun anecdote, I had an interview with a professor whose lab I was considering joining during my PhD applications. At that point, I had worked with four professors across several universities, both at my undergraduate institute (the University of Houston) as well as other universities. The professor I was interviewing introduced himself, and I quickly found out that this professor knew every almost every professor I had worked with either directly or indirectly; he had been faculty at a previous university with one of them, and knew the others through indirect research connections or by attending the same institutes. I learned that a major reason for him picking up my application out of the pool of emails he had received was that he had reason to believe in each of my research experiences, since he knew those professors and trusted their research, and by extension, the work I had done with them. <br/> <br/> This is a fun story for me, but it hints at something bigger - reference letters and indirect connections form a large part of what researchers rely on when gauging if a new student or worker will perform well in their lab. When given a pool of candidates, a professor will pick an equal (or possibly even less-qualified student on paper) if they have more reason to believe in that candidates experience, such as if they know the professor or department whom you worked with. </div> <p>Spending a few years working with different labs and gathering research experience goes a long way in preparing you for graduate school. As a final thought, it is important to see projects through to completion, meaning submission and publication in conferences or journals. Not every paper needs to be in a top conference or journal venue - you can publish much shorter workshop or symposium papers, which is totally fine! But you want to ensure that you reach publication and dissemination of your research ideas to the community through papers and code, so that you see the output of your work from each lab realized. As an undergraduate, my biggest regret is taking too long to realize that I myself can fight much more to have my smaller research projects published, which made me rush in my final year of undergrad to get publish or submit everything. Being your own biggest advocate is important, since other professors or graduate students will not necessarily be thinking about your research profile development as an undergrad.</p> <p>As a final thought, looking at the phases I have outlined above, it is apparent that working with multiple research labs and building up your research profile can take several years, depending on how many things you are able to work on concurrently. Getting started early in your undergraduate career is a big benefit, in order to give yourself time to complete research projects and work with multiple labs and professors. But if you end up getting into research later on, don’t worry - if I would give myself one piece of advice from a few years into the future, I would tell myself to take my time and give myself more years to complete research and build up my profile (rather than trying to cram everything into a few years as I did in my last 1.5 years of undergrad). There are often postgraduate positions which you can take up after completing a bachelor’s degree but before making graduate applications, which pays a decent stipend and gives you an extra year or two to build up more research. I only learned that this was a good option after I had joined graduate school and interacted with more people who took different career paths to get into PhD programs.</p> <h1 id="phase-4-how-to-reach-out-during-phd-applications">Phase 4: How to Reach Out During PhD Applications</h1> <p>Now that you’ve built up your research profile, it is time to make applications for PhD programs. Applications can be a stressful and uncertain period, and it is difficult to get any guarantees on whether your application will be accepted or not. The applications which worked out best for me during my own graduate applications was situations where I knew professors at my target university and had some connection with them beforehand, either by directly working with them as an intern, through interviews in the fall semester during application season, or through indirect connections.</p> <p>PhD applications usually require you to specify a few professors at that university whose research you are interested in, and whom you would potentially like to work with. The selection is important - these are the professors who can potentially pick up your application out of the pool of candidates, so you want to select professors who have similar research topics that you’ve worked in and who are likely to be accepting students (i.e. young professors who are growing their lab). Regarding the timeline for applications and interviews, one thing which I found helpful to do was:</p> <ul> <li>A few months before I submit my application to the university (e.g. 2-3 months before December or January deadlines), I would email professors whom I intend to mention in my application. If it is a professor whom I have never worked withI would notify them politely that I am submitting my application and intend on mentioning them as a potential advisor since their research is interesting and fits my background. This gives a chance for the professor to either: <ul> <li>(1) Let you know in advance that they will not be able to accept students, which frees you to select another professor to improve your chances of selecting professors who might be looking for applications.</li> <li>(2) or, schedule an interview with you if they find your profile interesting. This is the second way I have found to get in touch with professors. Interviews can range widely, but usually they involve presenting about your prior research work and experiences to the professor or one of their graduate students.</li> </ul> </li> </ul> <p>As a final thought, there is an uncertain time period where you have submitted applications and are waiting for results for several months, and it is unclear where you will get acceptances or not. Gauging your relative chances against other applicants can be difficult because of the numerous factors involved in graduate applications, but I will write out at a high-level my own applications and outcomes in hopes that it might inform and encourage other students who are applying.</p> <p>Applications (ranked by relative connection to that university):</p> <ul> <li><strong>Tier 1:</strong> After working as a undergraduate research intern across a few labs, I submitted applications to two Computer Science graduate programs knowing that I already worked with at least one professor at that University (including the graduate program I am at now, at Yale University).</li> <li><strong>Tier 2:</strong> There were four more graduate programs I applied to where I did not directly work with any professor at the university, but I had reached out 1-2 months beforehand introducing, which led to strong interviews with those professors.</li> <li><strong>Tier 3:</strong> In remaining two graduate programs I applied to, I had not worked with any professors directly and did not get email responses from any of the professors. My research area (mainly graph representation learning), however, was related enough to the work of several professors there that it would be a decent research fit.</li> </ul> <p>Outcomes:</p> <ol> <li>I received offers from one of my “Tier 1” universities, which I ended up accepting. At the other university, where the admissions committee of the university had stronger say relative to individual professors on graduate admissions, I did not receive an offer, which goes to show that nothing is ever completely guaranteed!</li> <li>I received offers from both of my “Tier 2” universities. The connections with both professors from the 2 graduate programs was strong in hindsight, in that those professors trusted my undergraduate research work (they knew my undergraduate advisors indirectly or directly), and my research area highly aligned with what they were working on. It is no coincidence that these two professors responded to my email when I reached out to them, and chose to interview me. My takeaway from this in hindsight is that you don’t need to work with every professor before applying to their lab as a PhD student; there are other ways you can find good research fit, if you select professors to reach out to carefully.</li> <li>I did not receive any offers from “Tier 3” universities, likely because I did not generate enough interaction with any professors at those institutes, and other professors were not looking for students with my research background.</li> </ol> <h1 id="closing-thoughts">Closing thoughts</h1> <p>I hope that this post contained useful information for organizing and strategizing about your own graduate applications! As an undergraduate myself, what often helped the most was hearing what other PhD students did to reach out and generate interaction with professors, since it was not immediately obvious to me that I could take certain steps (e.g. reaching out to professors months or years in advance to submitting my PhD application) that would greatly increase my chances at getting accepted into some PhD program.</p>]]></content><author><name></name></author><category term="Research"/><category term="research"/><summary type="html"><![CDATA[Things that I learned as an undergraduate research student]]></summary></entry><entry><title type="html">Basics of Graph Neural Networks</title><link href="https://syeda5688.github.io/blog/2024/gnn-basics/" rel="alternate" type="text/html" title="Basics of Graph Neural Networks"/><published>2024-06-09T03:00:00+00:00</published><updated>2024-06-09T03:00:00+00:00</updated><id>https://syeda5688.github.io/blog/2024/gnn-basics</id><content type="html" xml:base="https://syeda5688.github.io/blog/2024/gnn-basics/"><![CDATA[<p>This guide is a short, intuitive introduction to Graph Neural Networks (GNNs), specifically Message-Passing Neural Networks, aimed for students and researchers looking to learn more about training basic neural networks on graph-structured data. I’ve had many great conversations with labmates and friends who are looking to understand GNNs more deeply, yet find it hard to get to the crux of how these models learn on graphs. In that spirit, I’m collecting some thoughts, perspectives, and pseudocode which helped me understand Graph Neural Networks more deeply when I first started studying them as an undergraduate student.</p> <p>As a note beforehand, this guide is not meant as a comprehensive review or in-depth tutorial on GNNs; rather, it is meant to build intuition for what is happening under the hood of simple GNNs. Our goal by the end will be to have the ability to point at any operation inside the GNN and explain what it is doing, and what are the shapes and meaning of all the tensors and neural network weights involved. A follow-up blog post will relate the pseudocode shown at the end to real Python and Pytorch Geometric code.</p> <h1 id="graphs-all-around-us">Graphs, all around us!</h1> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog-20240609-gnn-basics-1-graph_structured_data-480.webp 480w,/assets/img/blog-20240609-gnn-basics-1-graph_structured_data-800.webp 800w,/assets/img/blog-20240609-gnn-basics-1-graph_structured_data-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog-20240609-gnn-basics-1-graph_structured_data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Images were generated using DALL-E and ChatGPT. </div> <p>Data in the world often comes associated with some sort of underlying structure. For example, images come with a 2D grid structure, which allows us to group and analyze pixels within local regions together. We can make assumptions about the data and build these into our neural network architectures in the form of <strong>inductive biases</strong>, which helps the model learn and generalize on the data. Weight sharing and spatial locality in Convolutional Neural Networks (CNNs) are great examples of this.</p> <p>Oftentimes, however, the data is structured in a more varied way, with entities connected to one another by relationships in real life. For example, humans are connected to one another in social networks through friendship connections and online interactions, which might be represented as a graph by defining each user as a <strong>node</strong> connected to other users by <strong>edges</strong> which represent online connections or interactions through posts. Molecular graphs connect atoms to other atoms through different types of chemical bonds, and a road network might define different cities as nodes which are connected to one another by a web of roads. Any one <strong>“node”</strong> entity in these graphs may be connected to any number of other entities through <strong>edge</strong> connections, which means that any neural network we design to learn on this graph-structured data will need to have a very generalized aggregation scheme to effectively integrate information from nodes and their surrounding neighbors. What is the benefit of representing this real-world data as a graph, rather than some other conventional data format? It allows us to flexibly model relationships between any number of entities connected by any number of edges, without having to simplify or project our data into a simpler format.</p> <p>Furthermore, the <span style="color:green">entities</span> and <span style="color:orange">relationships</span> we define in our graphs can capture more complexities which we find in real-world data. In social media platforms, for example, think about <span style="color:green">individual users</span> and <span style="color:green">companies</span> both being considered users on the platform, who can <span style="color:orange">write</span> <span style="color:green">posts</span> and be <span style="color:orange">a part of</span> <span style="color:green">subcommunities</span>. All of these can be defined as separate types of nodes and edges, with different associated feature attributes. We can even have multi-hop relationships (e.g. <span style="color:orange">a friend</span> of <span style="color:orange">a friend</span>), which can make for some fascinating modeling challenges! We’ll leave that for another post, and stick to basic <strong>homogeneous graphs</strong> for now, where we deal with only one type of entity.</p> <h1 id="how-do-we-represent-graphs">How do we represent graphs?</h1> <p>We’ve seen examples of graph-structured data, however we need a principled way of representing the feature attributes and connectivity information of a graph in matrices, so that we can do operations on them and learn from data using neural networks. Let’s define a few matrices which will tell us how we hold the graph data, namely the <strong>node feature matrix</strong> and the <strong>adjacency matrix</strong>:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix-480.webp 480w,/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix-800.webp 800w,/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For now, let’s keep looking at a small molecular graph from earlier, made up of six blue and green atoms numbered 1 to 6. We have two matrices which hold all of the information we need to describe a simple graph, so let’s take a closer look and understand what is in each matrix.</p> <h2 id="node-feature-matrix">Node feature matrix</h2> <ul> <li>The node feature matrix is a matrix which contains all of the features for all nodes in our graph. The shape of this matrix will be [number_of_nodes x number of features], which is [6 x 4] in our small example above, and is usually denoted as \(X\). With \(N=6\) nodes and \(F=4\) features, we have \(X \in R^{N \times F}\). You can imagine that the four features might be attributes of each atom, such as its atomic number, atomic mass, charge, and other relevant attributes.</li> </ul> <h2 id="adjacency-matrix">Adjacency matrix</h2> <ul> <li>The adjacency matrix is a (usually) binary matrix which contains information about what nodes are connected to what other nodes in the graph. This helps us keep track of connections, which we will need once we define a neural network architecture to aggregate information from the surrounding neighbors of each node. Information aggregation in graphs is useful because learning on graphs involves both understanding nodes as well as how they interact with and are similar to their neighboring nodes.</li> <li>The shape of the adjacency matrix will be [number_of_nodes x number_of_nodes], which will be [6 x 6] in our small example and is usually denoted as \(A \in R^{N \times N}\). Edges usually have some directionality (a “source” node and “destination” node), so by convention we say that source nodes are the rows and destination nodes are the columns of the matrix, with a 1 indicating an edge between source node \(u\) and destination node \(v\).</li> <li>You’ll notice that the diagonal of the adjacency matrix are all 1s, and are highlighted in <span style="color:green">green</span>. We have a choice in modeling our graph of whether we want to consider a node as connected to itself or not (it may or may not make a difference depending on our data and GNN architecture). For cases where a node’s features or state affects its own state in the future (i.e. an atom’s embedding should reflect the atom’s identity along with other atoms it is connected to), it is generally good to include self-loops. For this simple example, we will include self-connections to connect atoms to themselves.</li> <li>You will also notice that the adjacency matrix is symmetric around its diagonal; this means we are working on an undirected graph (atom 1 being connected to atom 2 means 2 is connect to 1 as well). This is not always the case, for example, think about a citation networks: paper A citing paper B does not mean the reverse is true.</li> </ul> <p>With these two matrices, we have everything we need to numerically describe our graph-structured data. The node feature matrix \(X\) can be seen as initial/input node features, and our goal for learning on graphs will be to learn node embeddings \(H \in R^{N \times D}\), where \(D\) is some hidden dimension which we choose, which meaningfully represent each node for downstream tasks based on both the node’s input features and the neighboring nodes it was connected to. Downstream tasks may include <strong>node-level</strong> tasks such as classifying what type of atom each node is, <strong>edge-level</strong> tasks such as classifying what bond type two atoms should have between one another, and <strong>graph-level</strong> tasks such as predicting whether the molecule as a whole is toxic or not. You can imagine how, depending on the task, it is important for each atom to integrate information from neighboring atoms and have an overall picture of where it is in relation to the whole molecule.</p> <h1 id="learning-on-graphs-graph-neural-networks">Learning on Graphs: Graph Neural Networks</h1> <p>Now that we’ve seen our data and represented it using node feature and adjacency matrices, let’s get into actually learning on graph-structured data. Because graph data varies in both number of nodes and edge connections between nodes, we need a neural network architecture which can operate on arbitrary node entities with variable number of neighbors while producing meaningful node embeddings for our task. On images, we usually perform information aggregation by taking advantage of spatial locality in images, convolving over groups of pixels to form higher-level abstract features. On graphs, however, we are going to define a <strong>graph convolution</strong>, which aggregates information from a node and all of its neighbors, and updates that node’s learned embedding in a message-passing step.</p> <p>Many GNN architectures have been proposed with varying forms of graph convolutions, and several of the simple, classic GNNs are still used (Graph Convolutional Networks (GCNs) [1], GraphSAGE [2], and Graph Attention Networks [3], to name a few). When learning about GNNs, however, it can be helpful to first start with thinking simply about <strong>message-passing neural networks (MPNNs)</strong>, which is an abstraction of GNN frameworks for learning on graphs proposed in [4]. MPNNs are a general framework where nodes pass messages to one another along edges in the graph in three defined steps:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog-20240609-gnn-basics-3-message-passing-480.webp 480w,/assets/img/blog-20240609-gnn-basics-3-message-passing-800.webp 800w,/assets/img/blog-20240609-gnn-basics-3-message-passing-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog-20240609-gnn-basics-3-message-passing.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li><strong>Message:</strong> every node decides how to send information to neighboring nodes it is connected to by edges</li> <li><strong>Aggregate:</strong> nodes receive messages from all of their neighbors, who also passed messages, and decides how to combine the information from all of its neighbors</li> <li><strong>Update:</strong> each node decides how to combine neighborhood information with its own information, and updates its embedding for the next timestep</li> </ol> <p>If we can define these three operations, then we can have all nodes pass each other information in what is considered one message passing step, which disseminates information around the graph a bit. This can be repeated for \(K\) iterations, and the more times we pass information around (larger \(K\)), the more we diffuse information around the graph, which affects the embeddings we get at the end. One way I like to think about this is a group of people spaced 1 step apart from each other, iteratively telling those next to them their name + any other names they have heard from their neighbors. After K rounds of name-telling (information-passing), any one person will have heard the name of all people within K steps of them at least once.</p> <p>Finally, if we incorporate some learned weights from a neural network into our message-passing operations and define a loss function on the resulting embeddings for some downstream task (e.g. node classification), then we have all of the ingredients we need for learning on graph-structured data.</p> <p>Let’s zoom in a bit on each step for one destination node \(v\), define some notation, and visualize how the node feature matrix and adjacency matrix are going into each operation:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog-20240609-gnn-basics-4-operations-480.webp 480w,/assets/img/blog-20240609-gnn-basics-4-operations-800.webp 800w,/assets/img/blog-20240609-gnn-basics-4-operations-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog-20240609-gnn-basics-4-operations.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ol> <li><strong>Message:</strong> source node \(u\) will pass message \(m_{uv}\) to destination node \(v\), which is node 2 in our small example. <ul> <li>What exactly is the message? It depends on the GNN architecture! For simplicity, we will go with the easiest message node \(u\) can give to node \(v\), which is just passing its node feature \(h_u\) vector to \(v\). More complex GNNs might do some learned operations to come up with a better message.</li> </ul> </li> <li><strong>Aggregate:</strong> we can choose some aggregation function to combine information from neighboring nodes, such as SUM or MEAN, which works across any number of neighboring nodes. This gives us a combined neighborhood node embedding denoted as \(h_{N(v)}\), where \(N(v)\) denotes the neighborhood of destination node \(v\), meaning all nodes connected to node \(v\). <ul> <li> \[h_{N(v)}^{k+1} = AGGREGATE({h_u^k, \forall u \in N(v)})\] </li> <li>Note: a special note about the aggregate operation is that we usually need to choose a permutation-invariant function to aggregate neighboring node messages. This because neighboring nodes don’t have an ordering with respect to the destination node, so our aggregate function needs to give the same output no matter the ordering of the inputs.</li> </ul> </li> <li><strong>Update:</strong> we can concatenate the neighborhood embedding \(h_{N(v)}^{k+1}\) with the embedding of the node itself, \(h_v^k\), and parameterize it with some learned weights \(W\) and a nonlinearity \(\sigma\) to form our final update step: <ul> <li> \[h_v^{k+1} = \sigma(W \cdot CONCAT(h_v^k, h_{N(v)}^{k+1}))\] </li> </ul> </li> </ol> <p>And now we’ve done it! We’ve made it through one message passing step, and if we repeat this for all destination nodes v, then we have our updated node embeddings for the next timestep \(k+1\).</p> <h1 id="a-general-algorithm-for-message-passing">A general algorithm for message-passing</h1> <p>The GraphSAGE paper [2] introduces a pseudocode algorithm for message passing which I quite like, and will put below for those thinking about the overall algorithm. This is actually the first algorithm I dissected as an undergraduate student to understand each operation and relate it to code implementations (which I will do in another blog post!).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog-20240609-gnn-basics-5-graphsage-alg-480.webp 480w,/assets/img/blog-20240609-gnn-basics-5-graphsage-alg-800.webp 800w,/assets/img/blog-20240609-gnn-basics-5-graphsage-alg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog-20240609-gnn-basics-5-graphsage-alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is quite a powerful algorithm when you think about it: in one code block, containing 10 lines, we can define a sequence of operations that encompasses how all MPNNs operate on arbitrary graph-structured data, and can become arbitrarily complex depending on how you define each of the three core operations: <strong>message</strong>, <strong>aggregate</strong>, and <strong>update</strong>.</p> <h1 id="connecting-things-together">Connecting things together</h1> <p>The nice thing about thinking through the message-passing framework is that we can recover many classical GNN architectures depending on the choice of message, aggregate, and update operations. Here are a few examples I like to think of (simplifying a bit for the sake of explanation):</p> <ul> <li>If we choose our permutation-invariant aggregator to be a simple averaging, and include self-connections in our adjacency matrix, we can recover the original GCN architecture [1]. The GCN formulation defines this as a matrix multiplication: \(\tilde{A}XW\), which does the aggregation through matrix multiplication with a normalized adjacency matrix \(\tilde{A}\).</li> <li>In the message step, what if we consider how much the source node is important to the destination node, and assign a score for that edge? We could weigh the edges with these scores if we normalize them correctly, for example by making all incoming edge scores sum up to 1. Then, our aggregation is a weighted aggregation, which is more informative than assuming all neighboring nodes have the same importance. This is the main idea behind GATs [3].</li> </ul> <p><strong>Final note:</strong> thank you for reading through to the end of this blog post! I appreciate your attention, and hope these ideas are useful to you in your work or studies as much as it was useful for me when I began studying GNNs. As this is my first blog post, I’d greatly appreciate any comments/tips/suggestions! The best place to reach me is at my email: syed [dot] rizvi [at] yale [dot] edu.</p> <h1 id="references">References</h1> <ol> <li>Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv preprint arXiv:1609.02907 (2016).</li> <li>Hamilton, Will, Zhitao Ying, and Jure Leskovec. “Inductive representation learning on large graphs.” Advances in neural information processing systems 30 (2017).</li> <li>Veličković, Petar, et al. “Graph attention networks.” arXiv preprint arXiv:1710.10903 (2017).</li> <li>Gilmer, Justin, et al. “Neural message passing for quantum chemistry.” International conference on machine learning. PMLR, 2017.</li> </ol>]]></content><author><name></name></author><category term="Graph-Neural-Networks"/><category term="pseudocode"/><summary type="html"><![CDATA[Intuitive introduction to message-passing Graph Neural Networks]]></summary></entry><entry><title type="html">Coding GraphSAGE From Scratch</title><link href="https://syeda5688.github.io/blog/2024/graphsage-from-scratch/" rel="alternate" type="text/html" title="Coding GraphSAGE From Scratch"/><published>2024-06-09T03:00:00+00:00</published><updated>2024-06-09T03:00:00+00:00</updated><id>https://syeda5688.github.io/blog/2024/graphsage-from-scratch</id><content type="html" xml:base="https://syeda5688.github.io/blog/2024/graphsage-from-scratch/"><![CDATA[<p>In the previous blog post, we took a short, intuitive look at the basics of Graph Neural Networks (GNNs), and at the end got a look at some pseudocode for a popular classical GNN architecture, GraphSAGE. In this post, we will go through a from-scratch Python implementation of the entire GraphSAGE algorithm, building up each step of message passing and connecting real lines of Pytorch code to lines of the original pseudocode algorithm.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog-20240609-gnn-basics-5-graphsage-alg-480.webp 480w,/assets/img/blog-20240609-gnn-basics-5-graphsage-alg-800.webp 800w,/assets/img/blog-20240609-gnn-basics-5-graphsage-alg-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog-20240609-gnn-basics-5-graphsage-alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As a quick note: since we are writing our code from scratch for understanding, it will end up being a bit more verbose than necessary. In practice, many operations are abstracted away, hidden under-the-hood by GNN libraries like Pytorch Geometric (PyG) [1], allowing us to just focus on the details of our GNN and data which we care about. Our code will not use any data structures or layers from PyG for simplicity, so that only an understanding of Pytorch and Python class-based definitions is necessary to read the code snippets.</p> <p>The goal by the end of this coding tutorial is to feel comfortable looking at code implementations of PyG-style message-passing. Afterwards, the jump to looking at real source code of different GNNs in PyG (<a href="https://pytorch-geometric.readthedocs.io/en/latest/index.html">docs</a>) will feel easier, which will enable you to read more GNN papers and make more connections to real code! If you end up using an alternative GNN library other than PyG in your research (or an alternative library to Pytorch, or another programming language altogether!), don’t worry, the general idea of message-passing operations should carry over sufficiently well to other implementations.</p> <h1 id="starting-out-input-graph-information">Starting out: Input graph information</h1> <p>To start out, let’s revisit the molecular graph example which we saw in the previous post:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix-480.webp 480w,/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix-800.webp 800w,/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog-20240609-gnn-basics-2-node-feat-adj-matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We will build our graph input example from this example, for familiarity. The graph will consist of 6 nodes representing hydrogen (blue) and carbon (C) atoms, each with four features: atomic number, atomic mass, (made-up) charge values, and number of incoming edges. We also have the same adjacency matrix from before, with 18 edges in total connecting nodes together, including self-edges.</p> <p>We can initialize our node feature matrix and adjacency matrix as Pytorch tensors in our Python code as follows:</p> <pre><code class="language-Python"># Define input node feature matrix and adjacency matrix
input_node_feature_matrix = torch.tensor([
    [1.0, 1.0078, 1, 2],  # atomic number, atomic mass, charge, and number of bonds
    [1.0, 1.0078, 1, 4],
    [6.0, 12.011, -1, 3],
    [1.0, 1.0078, 0, 4],
    [6.0, 12.011, -1, 3],
    [1.0, 1.0078, 1, 2],
], dtype=torch.float32)  # [num_nodes, num_features]

binary_adjacency_matrix = torch.tensor([
    [1, 1, 0, 0, 0, 0],
    [1, 1, 1, 0, 1, 0],
    [0, 1, 1, 1, 0, 0],
    [0, 0, 1, 1, 1, 1],
    [0, 1, 0, 1, 1, 0],
    [0, 0, 0, 1, 0, 1],
], dtype=torch.int64)  # [num_nodes, num_nodes]
</code></pre> <p>These two Pytorch tensors contain all of the information we need about node features as well as graph connectivity. You’ll notice, however, that the adjacency matrix contains many zero values, which gets worse as we scale to much larger graphs (millions and billions of nodes!) since nodes tend to be connected to only a few other nodes.</p> <p>Because of this, we often opt for an edge list representation of graph connectivity information, where instead of a [num_nodes, num_nodes] matrix containing many zeros for missing edges, we transform it into an edge list of shape [2, num_edges], which specifies the index of the start (source) and end (destination) node for each existing edges. With this representation, we only store two pieces of information for edges that actually exist, rather than 1 piece of information for every possible edge that might exist in the graph. Libraries such as PyG opt for this edge list representation, which they call an <strong>edge_index</strong>, so we will define a conversion function ourselves to turn an adjacency matrix into an edge_index tensor as follows:</p> <pre><code class="language-Python">def adj_matrix_to_sparse_edge_index(adj_matr: torch.Tensor):
    """
    This function takes a square binary adjacency matrix, and returns an edge list representation
    containing source and destination node indices for each edge.

    Arguments:
        adj_matr: torch Tensor of adjacency information, shape [num_nodes, num_nodes], dtype torch.int64
    Returns:
        edge_index: torch Tensor of shape [2, num_edges], dtype torch.int64
    """
    src_list = []
    dst_list = []
    for row_idx in range(adj_matr.shape[0]):
        for col_idx in range(adj_matr.shape[1]):
            if adj_matr[row_idx, col_idx].item() &gt; 0.0:
                src_list.append(row_idx)
                dst_list.append(col_idx)
    return torch.tensor([src_list, dst_list], dtype=torch.int64)  # [2, num_edges]

edge_index = adj_matrix_to_sparse_edge_index(binary_adjacency_matrix)  # [2, num_edges]
</code></pre> <h1 id="defining-our-message-passing-layer">Defining our message-passing layer</h1> <p>Now that we have our input node feature and edge_index tensors, we can move on to defining our message-passing layer which will implement the GraphSAGE message-passing algorithm. If we look at the pseudocode at the beginning, we can see that in the main message-passing logic happens in two lines of pseudocode, which happen for each node in the graph:</p> <ul> <li> \[h_{N(v)}^{k+1} = AGGREGATE({h_u^k, \forall u \in N(v)})\] </li> <li> \[h_v^{k+1} = \sigma(W \cdot CONCAT(h_v^k, h_{N(v)}^{k+1}))\] </li> </ul> <p>These two lines of code define mathematically how we will do message passing, specifying the steps of message-passing which we previously covered: assuming (1) messages have been created, we (2) aggregate messages from neighboring nodes to get \(h_{N(v)}^{k+1}\), and (3) update representations, ending up with \(h_v^{k+1}\).</p> <p>To implement this in code, we will need an organized definition of the message, aggregate, and update steps. In Pytorch-style coding, neural network layers are typically defined in Python class syntax, where we define a Python class which will house our GNN layer:</p> <pre><code class="language-Python">class GraphSAGELayer(nn.Module):
    def __init__(self, in_dim: int, out_dim: int):
        super().__init__()
        # Define linear layers parameterizing neighbors and self-loops
        self.lin_neighbors = nn.Linear(in_dim, out_dim, bias=True)
        self.lin_self = nn.Linear(in_dim, out_dim, bias=True)
</code></pre> <p>Here we have defined a Python class called GraphSAGELayer, which inherits from Pytorch’s torch.nn.Module class. This Module class lets us inherit functionalities for neural network that will allow us to train our model using stochastic gradient descent, along with all of Pytorch’s other functionalities.</p> <p>Looking again at the two pseudocode lines, we can see that the only learnable parameters in GraphSAGE is a weight matrix \(W\), which parameterizes a concatenation of a node’s own embedding \(h_v^k\) with its neighborhood embedding \(h_{N(v)}^{k+1}\). In practice, GraphSAGE is implemented in Pytorch Geometric (<a href="https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/sage_conv.html#SAGEConv">here</a>) using two linear layers for neighboring message embeddings and a node’s self embedding. The reason for this might be twofold: (i) having two separate layers leaves an option to not have a self-embedding weight, which can be desirable sometimes, and (ii) sometimes we may want separate weights parameterizing self-connections, which can be seen as a form of skip-connections for GNN embeddings.</p> <p>In our code, we will call these two linear layers <strong>lin_neighbors</strong> and <strong>lin_self</strong>, as shown above. Now comes an important part: how do we implement logic to create and pass messages, and aggregate embeddings for neighbors in order to obtain \(h_{N(v)}^{k+1}\)? We can define our own function for message-passing as follows:</p> <pre><code class="language-Python">    def message_passing(self, x: torch.Tensor, edge_index: torch.Tensor):
        """
        This function is responsible for passing messages between nodes according to the edges 
        in 'edge_index'.
        - Messages from the source --&gt; destination node consist of the source nodes feature vector.
        - Sum aggregation is used to aggregate incoming messages from neighbors.

        Arguments:
            x: torch Tensor of node representations, shape [num_nodes, hidden_size], dtype torch.float32
            edge_index: torch Tensor of graph connectivity information, shape [2, num_edges], dtype torch.int64
        Returns:
            neigh_embeds: torch Tensor of aggregated neighbor embeddings, shape [num_nodes, hidden_size], dtype torch.float32
        """
        src_node_indices = edge_index[0, :]  # shape [num_edges]
        dst_node_indices = edge_index[1, :]  # shape [num_edges]
        # Step (1): Message
        src_node_feats = x[src_node_indices]  # shape [num_edges, hidden_size]

        # Mean aggregation
        neighbor_sum_aggregations = []
        for dst_node_idx in range(x.shape[0]):  # loop over destination nodes
            # find incoming edges, get incoming messages from source nodes
            incoming_edge_indices = torch.where(dst_node_indices == dst_node_idx)[0]  # find incoming edges
            incoming_messages = src_node_feats[incoming_edge_indices]  # shape [num_incoming_edges, hidden_size]

            # Step (2): Aggregate - sum messages from neighbors (if &gt; 1 neighbors)
            incoming_messages_summed = incoming_messages.sum(dim=0) if incoming_messages.shape[0] &gt; 1 else incoming_messages
            neighbor_sum_aggregations.append(incoming_messages_summed)
        
        neigh_embeds = torch.stack(neighbor_sum_aggregations)  # [num_nodes, hidden_size]
        return neigh_embeds
</code></pre> <p>This function is quite involved, so we will go through step-by-step, and point out where code connects to pseudocode with comments. We can see that the inputs to this function are our node feature matrix <strong>x</strong>, and the <strong>edge_index</strong>, which we already have from earlier. The function definition states that we will take these two tensors as input, and we will eventually return \(h_{N(v)}^{k+1}\).</p> <p>The first thing we need to do is organize how our nodes are going to pass messages to each other, for Step (1): Message. The simplest message which one node can pass to another node is its node embedding (which is also the case in GraphSAGE), so we first get the indices of our source nodes from our <strong>edge_index</strong>, and use that to index into our node feature matrix <strong>x</strong>. If you are familiar with array indexing in Pytorch, you’ll realize that this gives us a [num_edges, hidden_size] tensor, effectively giving us a tensor containing source node embeddings. This is an important step, because with the leading dimension being <em>num_edges</em> rather than <em>num_nodes</em>, we can do edge operations and deal with passing messages along edges.</p> <p>With this indexing operation, our first step of message creation is already complete, since we are using source node embeddings as the message to be passed. Now, we need to perform the next step, which is to aggregate embeddings for each destination node using a permutation-invariant aggregator. We will use sum aggregation here, since it is a more expressive aggregation function (more on that another time!), which means for each destination node in the graph, we need to sum all incoming message embeddings. We accomplish this by looping over destination nodes, finding which edges are ending at that destination node, and summing the corresponding messages. The resulting variable, <strong>neigh_embeds</strong>, directly corresponds to \(h_{N(v)}^{k+1}\) in the pseudocode.</p> <p>We can complete our message-passing layer implementation by writing a forward() function, which tells Pytorch how we want a forward pass through our neural network to be implemented:</p> <pre><code class="language-Python">    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):
        """
        Implementation for one message-passing iteration for GraphSAGE.

        Arguments:
            x: torch Tensor of node representations, shape [num_nodes, hidden_size], dtype torch.float32
            edge_index: torch Tensor of graph connectivity information, shape [2, num_edges], dtype torch.int64
        Returns:
            out: torch Tensor of updated node representations, shape [num_nodes, hidden_size], dtype torch.float32
        """
        x_message_passing, x_self = x, x  # duplicate variables pointing to node features
        neigh_embeds = self.message_passing(x_message_passing, edge_index)
        neigh_embeds = self.lin_neighbors(neigh_embeds)
        
        x_self = self.lin_self(x_self)
        # # Step (3): Update - sum concatenation to update node representations
        out = neigh_embeds + x_self
        return out
</code></pre> <p>With the message_passing() function doing the heavy lifting, all we need to do in the forward() function is call the function message passing, and then perform step 3, which is updating node representations. This is done by running \(h_{N(v)}^{k+1}\) and \(h_v^k\) through their respective linear layers, and then concatenating them together. In practice, concatenation operations are done either through summing vectors together, or by joining two vectors together (resulting in a longer vector). I have not seen a preference for either method for concatenation in code implementations thus far.</p> <h1 id="completing-a-1-layer-graphsage-model">Completing a 1-layer GraphSAGE model</h1> <p>Now that we have defined a full message-passing class using simple operations, we can complete a full 1-layer GNN model by defining a second class which will use our just-completed message passing layer definition:</p> <pre><code class="language-Python">class GraphSAGEModel(nn.Module):
    def __init__(self, in_features: int, hidden_size: int, out_features: int, dropout: int = 0.1):
        super().__init__()
        self.input_proj = nn.Linear(in_features, hidden_size, bias=True)

        self.conv1 = GraphSAGELayer(in_dim=hidden_size, out_dim=hidden_size)
        self.act1 = nn.ReLU()
        self.drop1 = nn.Dropout(p=dropout)
        
        self.lin_out = nn.Linear(hidden_size, out_features, bias=True)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):
        """
        Forward pass implementation of 1-layer GraphSAGE model.
        
        Arguments:
            x: torch Tensor of input node features, shape [num_nodes, num_features], dtype torch.float32
            edge_index: torch Tensor of graph connectivity information, shape [2, num_edges], dtype torch.int64
        """
        x = self.input_proj(x)  # Input projection: [num_nodes, num_features] --&gt; [num_nodes, hidden_size]

        x = self.conv1(x, edge_index)  # Message-passing
        x = self.act1(x)
        x = self.drop1(x)
        
        x = self.lin_out(x)
        return F.log_softmax(x, dim=-1)  # softmax over last dim for classification
</code></pre> <p>This class again inherits from nn.Module, and it defines 1 layer of message-passing by calling the GraphSAGELayer() class we just defined above. It also defines several other components, such as a ReLU nonlinearity after the message-passing layer, a dropout layer, and input/output projections. This definition is for a classification model with 1-message passing layer; if we wanted to change the task the model is built for, we could change the output head and remove the final softmax layer as we need depending on our task. If we have a need to pass messages multiple times, we can simply define more layers of our GraphSAGELayer class to pass messages more times! Note that this would mean not sharing weights for different message-passing iterations, which is common practice.</p> <h1 id="putting-everything-together">Putting everything together</h1> <p>We can now put everything together by defining an instance of our 1-layer GraphSAGE model and doing a full forward pass on our example graph! The full code is below, and is also available on <a href="https://github.com/SyedA5688/blog_post_tutorials/blob/master/graphsage_tutorial.py">GitHub</a>:</p> <pre><code class="language-Python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data


class GraphSAGELayer(nn.Module):
    def __init__(self, in_dim: int, out_dim: int):
        super().__init__()
        # Define linear layers parameterizing neighbors and self-loops
        self.lin_neighbors = nn.Linear(in_dim, out_dim, bias=True)
        self.lin_self = nn.Linear(in_dim, out_dim, bias=True)

    def message_passing(self, x: torch.Tensor, edge_index: torch.Tensor):
        """
        This function is responsible for passing messages between nodes according to the edges 
        in 'edge_index'.
        - Messages from the source --&gt; destination node consist of the source nodes feature vector.
        - Sum aggregation is used to aggregate incoming messages from neighbors.

        Arguments:
            x: torch Tensor of node representations, shape [num_nodes, hidden_size], dtype torch.float32
            edge_index: torch Tensor of graph connectivity information, shape [2, num_edges], dtype torch.int64
        Returns:
            neigh_embeds: torch Tensor of aggregated neighbor embeddings, shape [num_nodes, hidden_size], dtype torch.float32
        """
        src_node_indices = edge_index[0, :]  # shape [num_edges]
        dst_node_indices = edge_index[1, :]  # shape [num_edges]
        # Step (1): Message
        src_node_feats = x[src_node_indices]  # shape [num_edges, hidden_size]

        # Mean aggregation
        neighbor_sum_aggregations = []
        for dst_node_idx in range(x.shape[0]):  # loop over destination nodes
            # find incoming edges, get incoming messages from source nodes
            incoming_edge_indices = torch.where(dst_node_indices == dst_node_idx)[0]  # find incoming edges
            incoming_messages = src_node_feats[incoming_edge_indices]  # shape [num_incoming_edges, hidden_size]

            # Step (2): Aggregate - sum messages from neighbors (if &gt; 1 neighbors)
            incoming_messages_summed = incoming_messages.sum(dim=0) if incoming_messages.shape[0] &gt; 1 else incoming_messages
            neighbor_sum_aggregations.append(incoming_messages_summed)
        
        neigh_embeds = torch.stack(neighbor_sum_aggregations)  # [num_nodes, hidden_size]
        return neigh_embeds

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):
        """
        Implementation for one message-passing iteration for GraphSAGE.

        Arguments:
            x: torch Tensor of node representations, shape [num_nodes, hidden_size], dtype torch.float32
            edge_index: torch Tensor of graph connectivity information, shape [2, num_edges], dtype torch.int64
        Returns:
            out: torch Tensor of updated node representations, shape [num_nodes, hidden_size], dtype torch.float32
        """
        x_message_passing, x_self = x, x  # duplicate variables pointing to node features
        neigh_embeds = self.message_passing(x_message_passing, edge_index)
        neigh_embeds = self.lin_neighbors(neigh_embeds)
        
        x_self = self.lin_self(x_self)
        # # Step (3): Update - sum concatenation to update node representations
        out = neigh_embeds + x_self
        return out
        


class GraphSAGEModel(nn.Module):
    def __init__(self, in_features: int, hidden_size: int, out_features: int, dropout: int = 0.1):
        super().__init__()
        self.input_proj = nn.Linear(in_features, hidden_size, bias=True)

        self.conv1 = GraphSAGELayer(in_dim=hidden_size, out_dim=hidden_size)
        self.act1 = nn.ReLU()
        self.drop1 = nn.Dropout(p=dropout)
        
        self.lin_out = nn.Linear(hidden_size, out_features, bias=True)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):
        """
        Forward pass implementation of 1-layer GraphSAGE model.
        
        Arguments:
            x: torch Tensor of input node features, shape [num_nodes, num_features], dtype torch.float32
            edge_index: torch Tensor of graph connectivity information, shape [2, num_edges], dtype torch.int64
        """
        x = self.input_proj(x)  # Input projection: [num_nodes, num_features] --&gt; [num_nodes, hidden_size]

        x = self.conv1(x, edge_index)  # Message-passing
        x = self.act1(x)
        x = self.drop1(x)
        
        x = self.lin_out(x)
        return F.log_softmax(x, dim=-1)  # softmax over last dim for classification


def adj_matrix_to_sparse_edge_index(adj_matr: torch.Tensor):
    """
    This function takes a square binary adjacency matrix, and returns an edge list representation
    containing source and destination node indices for each edge.

    Arguments:
        adj_matr: torch Tensor of adjacency information, shape [num_nodes, num_nodes], dtype torch.int64
    Returns:
        edge_index: torch Tensor of shape [2, num_edges], dtype torch.int64
    """
    src_list = []
    dst_list = []
    for row_idx in range(adj_matr.shape[0]):
        for col_idx in range(adj_matr.shape[1]):
            if adj_matr[row_idx, col_idx].item() &gt; 0.0:
                src_list.append(row_idx)
                dst_list.append(col_idx)
    return torch.tensor([src_list, dst_list], dtype=torch.int64)  # [2, num_edges]


if __name__ == "__main__":
    # Define input node feature matrix and adjacency matrix
    input_node_feature_matrix = torch.tensor([
        [1.0, 1.0078, 1, 2],  # atomic number, atomic mass, charge, and number of bonds
        [1.0, 1.0078, 1, 4],
        [6.0, 12.011, -1, 3],
        [1.0, 1.0078, 0, 4],
        [6.0, 12.011, -1, 3],
        [1.0, 1.0078, 1, 2],
    ], dtype=torch.float32)

    binary_adjacency_matrix = torch.tensor([
        [1, 1, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0],
        [0, 1, 1, 1, 0, 0],
        [0, 0, 1, 1, 1, 1],
        [0, 1, 0, 1, 1, 0],
        [0, 0, 0, 1, 0, 1],
    ], dtype=torch.int64)
    edge_index = adj_matrix_to_sparse_edge_index(binary_adjacency_matrix)
    print("input_node_feature_matrix:", input_node_feature_matrix.shape)
    print(input_node_feature_matrix)
    print("binary_adjacency_matrix:", binary_adjacency_matrix.shape)
    print(binary_adjacency_matrix)
    print("edge_index:", edge_index.shape)
    print(edge_index, "\n")

    # Define GraphSAGE model
    model = GraphSAGEModel(
        in_features=4,  # 4 input features per node
        hidden_size=16,  # 16-dimensional latent vectors
        out_features=2  # 2 classes of nodes in our example: Carbon and Hydrogen
    )
    print("\nModel:")
    print(model, "\n")

    # Forward pass &amp; loss calculation for node classification
    output = model(x=input_node_feature_matrix, edge_index=edge_index)
    atom_labels = torch.tensor([0, 0, 1, 0, 1, 0], dtype=torch.int64)  # 0 = Hydrogen, 1 = Carbon
    loss = F.nll_loss(output, target=atom_labels)
    print("Loss value: {:.5f}".format(loss.item()))
</code></pre> <h1 id="wrapping-up">Wrapping up</h1> <p>I hope this code tutorial was useful for you! Many of these operations are abstracted away under the hood of GNN libraries, however understanding the underlying operations going on during message-passing the first step to being able to adapt and improve the algorithm as per your needs and goals. If the code snippets make sense, and you succeed in running them and looking at the printed outputs, I would highly enourage you to look at real source code for GNNs in Pytorch Geometric, for instance the <a href="https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/sage_conv.html#SAGEConv">GraphSAGE implementation</a>. You will notice that PyG exposes certain functions, such as <strong>message()</strong>, allowing developers to override these functions to inject custom behavior during message-passing. It is a clever software engineering design that allows developers to build custom GNN models which still abstracting low-level operations away from us, like aggregating neighboring nodes based on edges.</p> <p>As always, feedback is welcome and appreciated on this code tutorial at: syed [dot] rizvi [at] yale [dot] edu</p> <h1 id="references">References</h1> <ol> <li>Fey, Matthias, and Jan Eric Lenssen. “Fast graph representation learning with PyTorch Geometric.” arXiv preprint arXiv:1903.02428 (2019).</li> </ol>]]></content><author><name></name></author><category term="Graph-Neural-Networks"/><category term="code"/><summary type="html"><![CDATA[From-scratch Python implementation of GraphSAGE algorithm]]></summary></entry></feed>